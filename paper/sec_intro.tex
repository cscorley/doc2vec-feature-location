% vim:syntax=tex

Feature location is a frequent and fundamental activity for a developer tasked with changing a software system.
Whether a change task involves adding, modifying, or removing a feature, a developer cannot complete the task without first locating the source code that implements the feature.
The state-of-the-practice in feature location is to use an IDE tool based on keyword or regex search, but Ko et al.~\cite{Ko-etal:2006} observed such tools leading developers to failed searches nearly 90\% of the time.

The state-of-the-art in feature location~\cite{Dit-etal:2011} is to use a feature location technique (FLT) based, at least in part, on text retrieval (TR).
The standard methodology~\cite{Marcus-etal:2004} is to extract a document for each class or method in a source code snapshot, to train a TR model on those documents, and to create an index of the documents from the trained model.
Topics models (TMs)~\cite{Blei:2012} such as latent Dirichlet allocation (LDA)~\cite{Blei-etal:2003} are the state-of-the-art in TR and outperform vector-space models (VSMs) in the contexts of natural language~\cite{Deerwester-etal:1990,Blei-etal:2003} and source code~\cite{Poshyvanyk-etal:2007,Lukins-etal:2010}.
Yet, modern TMs such as online LDA~\cite{Hoffman-etal:2010} natively support only the online addition of a new document, whereas VSMs also natively support online modification or removal of an existing document.
So, TM-based FLTs provide the best accuracy, but unlike VSM-based FLTs, they require computationally-expensive retraining subsequent to source code changes.

Rao\cite{Rao:2013} proposed FLTs based on customizations of LDA and latent semantic indexing (LSI) that support online modification and removal.
%These FLTs require less-frequent retraining than others based on TMs.
These FLTs require less-frequent retraining than other TM-based FLTs,
but the remaining cost of periodic retraining inhibits their application to large software, and the reliance on customization hinders their extension to new TMs.

We envision an FLT that is: (1)~accurate like a TM-based FLT, (2)~inexpensive to update like a VSM-based FLT, and (3)~extensible to accommodate any off-the-shelf TR model that supports online addition of a new document.
Unfortunately, our vision is incompatible with the standard methodology for FLTs.
Existing VSM-based FLTs fail to satisfy the first criteria, and existing TM-based FLTs fail to satisfy the second or third criteria.
Indeed, given the current state-of-the-art in TR, it is impossible for an FLT to satisfy all three criteria while following the standard methodology.

In this paper we propose a new methodology for FLTs.
Our methodology is to extract a document for each changeset in the source code history and to train a TR model on the changeset documents, and then to extract a document for each class or method in a source code snapshot and to create an index of the class/method documents from the trained (changeset) model.
This new methodology stems from four key observations:
\begin{itemize}[leftmargin=*]
  \item
    Like a class/method definition, a changeset has program text.
  \item
    Unlike a class/method definition, a changeset is immutable.
  \item
    A changeset corresponds to a commit.
  \item
    An atomic commit involves a single feature.
\end{itemize}
It follows from the first two observations that it is possible for an FLT following our methodology to satisfy all three of the criteria above.
The next two observations influence the training and indexing steps of our methodology,
which have the conceptual effect of relating classes (or methods) to changeset topics.
By contrast, the training and indexing steps of the standard methodology
have the conceptual effect of relating classes to class topics (or methods to method topics).

To evaluate the new methodology, we used it to implement FLTs based on online LSI and online LDA.
We next used two benchmarks to compare the accuracy of these FLTs to the accuracy of comparable FLTs that follow the standard methodology.
Combined, the two benchmarks comprise over 1,200 defects and features from 14 open-source Java projects.
We then used a subset of over 600 defects and features to conduct a historical simulation that demonstrates how the FLTs perform as a project evolves.
Our evaluation results provide evidence that our new methodology is sound and indicate that FLTs following it provide similar accuracy to those following the standard methodology while eliminating retraining costs.

The remainder of the paper is organized as follows.
We first review background and related work (\S\ref{sec:related})
We next present our new methodology for FLTs (\S\ref{sec:changeset}) and report evaluation results for the online-LDA-based FLT (\S\ref{sec:study}).
We then conclude (\S\ref{sec:conclusion}).




\begin{comment}
Software developers are often confronted with maintenance tasks that involve
navigation of repositories that preserve vast amounts of project history.
Navigating these software repositories can be a time-consuming task, because
their organization can be difficult to understand.  A software developer who is
tasked with changing a large software system spends effort on program
comprehension activities to gain the knowledge needed to make the
change~\cite{Corbi:1989}.  Fortunately, topic models such as latent Dirichlet
allocation (LDA)~\cite{Blei-etal:2003} can help developers to navigate and
understand software repositories by discovering topics (word distributions) that
reveal the thematic structure of the
data~\cite{Linstead-etal:2007,Thomas-etal:2011,Hindle-etal:2014}.

One particular application of topic models is for \emph{feature location}.
Feature location is the act of identifying the source code that implements
a system feature.  The current state-of-the-practice for feature location is to
use a keyword search tool, such as \texttt{grep}.  Ko et al.~\cite{Ko-etal:2006}
show that developers fail using this type of searching upwards to 88\% of the
time.  Text retrieval techniques, such as topic modeling, show promise in
remedying this problem~\cite{Marcus-etal:2004}.

Typical topic-modeling-based feature location techniques (FLT) construct models
from corpora of text extracted from a source code
snapshot~\cite{Dit-etal:2011}.  To use a topic-modeling-based FLT, there are
generally two key steps: training and indexing.  In the first step, a corpus of
source code entities, such as methods or classes, are used to train the model to
learn word co-occurences within those entities.  The indexing step uses the
trained model to construct an index of the source code entities based on their
inferred topic distribution.  That is, an index is made of each source code's
\emph{thematic structure}, and not it's raw content.  Keeping such a model and
index up-to-date is expensive, because the frequency and scope of source code
changes, such as file removal, necessitate retraining the model on the updated
corpus and reindexing.  This situation is sub-optimal whether your perspective
is academic research or industrial tool-building.  Like Rao et
al.~\cite{Rao-etal:2013}, our primary research goal is elimination of this cost.
However, unlike Rao et al., we do not intend to develop new topic modeling
techniques, but rather use the existing ones.

In this paper, we propose a fresh take on topic-modeling-based FLTs by
leveraging online topic models and mining software repositories to construct
topic models that do not need retraining.  Online topic models do not need to
know the entire input corpus prior to
training~\cite{Hoffman-etal:2010}.  That is, online topic models can
be incrementally trained over time as more data becomes available.
Moreover, a version control repository, such as Git, keeps a history of source
code documents as they change over time.  These changes are represented as
changesets, which provide concise views of the differences between two revisions
of the same document.  By training an online topic model on changesets and
indexing the source code on that model, we can stream documents (i.e.,
changesets) from the version control repository to incrementally train the topic
model.  This enables searching over the current source code index without
retraining an entirely new model.

In our previous work~\cite{Corley-etal:2014}, we show that topic models trained
on changesets produce topics which have comparable topic distinctness
scores~\cite{Thomas-etal:2011} as topic models trained on snapshots.  Further,
we show that the corpora express the same frequency of words.  We expand the
work to demonstrate the effectiveness of changeset topic modeling for feature
location and report on an empirical study in which we investigate the
feasibility of this approach.
We define a LDA-based FLT using changesets.  We combine two benchmarks totaling
over 1200 defects and features from fourteen open source Java projects.  We also
present a \emph{historical simulation} that approximates how the FLT would perform
throughout the evolution of a project.

Our results show that the changeset approach is feasible and has performace
comparable to the snapshot approach.  In many cases the changeset approach
out-performs current snapshot approaches, but is no silver bullet.  We argue
that the evidence suggests that changeset-based topic modeling warrants further
investigation and adoption.  Additionally, the historical simulation suggests that
current evaluation approaches do not accurately capture the true FLT
performance.

This paper makes the following contributions:

\begin{itemize} \item An approach for using changesets for feature location
        \item A empirical study of fourteen open source Java projects \item
            Towards increasing open science principles in software engineering:
            the complete project history, source code, and an updated dataset
            for replication of this study.  \end{itemize}

The remainder of the paper is organized as follows.  We first review background
and related work (\S\ref{sec:related}) before introducing our new
changeset-based FLT (\S\ref{sec:changeset}).  We next discuss our case study
(\S\ref{sec:study}), which spans fourteen open source Java projects.  We then
conclude (\S\ref{sec:conclusion}).
\end{comment}

